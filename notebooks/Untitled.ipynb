{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5484404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from sklearn.model_selection import GridSearchCV,cross_validate\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec13bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a20364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    for punctuation in string.punctuation: \n",
    "        text = text.replace(punctuation, ' ').lower() \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c318fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers (text):\n",
    "    words_only = ''.join([i for i in text if not i.isdigit()])\n",
    "    return words_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac938a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Create function\n",
    "def remove_stopwords (text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    without_stopwords = [word for word in tokenized if not word in stop_words]\n",
    "    return without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a09368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer() # Initiate lemmatizer\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in text] # Lemmatize\n",
    "    lemmatized_string = \" \".join(lemmatized)\n",
    "    return lemmatized_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1610a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean (text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    return lemmatized\n",
    "\n",
    "# Apply to all texts\n",
    "# data['clean_text'] = data.text.apply(clean)\n",
    "# data['clean_text'] = data['clean_text'].astype('str')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
